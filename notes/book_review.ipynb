{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0d89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/bin/bash\n",
    "# !curl -L -o ~/datasets/amazon-books-reviews.zip\\\n",
    "#   https://www.kaggle.com/api/v1/datasets/download/mohamedbakhet/amazon-books-reviews\n",
    "\n",
    "# !pip install pandas \n",
    "# !which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad5801",
   "metadata": {},
   "source": [
    "# Discover book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcde617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db7b42d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   Id                  object \n",
      " 1   Title               object \n",
      " 2   Price               float64\n",
      " 3   User_id             object \n",
      " 4   profileName         object \n",
      " 5   review/helpfulness  object \n",
      " 6   review/score        float64\n",
      " 7   review/time         int64  \n",
      " 8   review/summary      object \n",
      " 9   review/text         object \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 228.9+ MB\n",
      "None\n",
      "\n",
      "First 3 rows:\n",
      "           Id                           Title  Price         User_id  \\\n",
      "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
      "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
      "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
      "\n",
      "             profileName review/helpfulness  review/score  review/time  \\\n",
      "0  Jim of Oz \"jim-of-oz\"                7/7           4.0    940636800   \n",
      "1          Kevin Killian              10/10           5.0   1095724800   \n",
      "2           John Granger              10/11           5.0   1078790400   \n",
      "\n",
      "                                    review/summary  \\\n",
      "0           Nice collection of Julie Strain images   \n",
      "1                                Really Enjoyed It   \n",
      "2  Essential for every personal and Public Library   \n",
      "\n",
      "                                         review/text  \n",
      "0  This is only for Julie Strain fans. It's a col...  \n",
      "1  I don't care much for Dr. Seuss but after read...  \n",
      "2  If people become the books they read and if \"t...  \n",
      "\n",
      "Basic Statistics:\n",
      "                Id       Title          Price         User_id  \\\n",
      "count      3000000     2999792  481171.000000         2438213   \n",
      "unique      221998      212403            NaN         1008972   \n",
      "top     B000IEZE3G  The Hobbit            NaN  A14OJS0VWMOSWO   \n",
      "freq          6796       22023            NaN            5795   \n",
      "mean           NaN         NaN      21.762656             NaN   \n",
      "std            NaN         NaN      26.206541             NaN   \n",
      "min            NaN         NaN       1.000000             NaN   \n",
      "25%            NaN         NaN      10.780000             NaN   \n",
      "50%            NaN         NaN      14.930000             NaN   \n",
      "75%            NaN         NaN      23.950000             NaN   \n",
      "max            NaN         NaN     995.000000             NaN   \n",
      "\n",
      "                profileName review/helpfulness  review/score   review/time  \\\n",
      "count               2438095            3000000  3.000000e+06  3.000000e+06   \n",
      "unique               854145              12084           NaN           NaN   \n",
      "top     Midwest Book Review                0/0           NaN           NaN   \n",
      "freq                   5817             885732           NaN           NaN   \n",
      "mean                    NaN                NaN  4.215289e+00  1.132307e+09   \n",
      "std                     NaN                NaN  1.203054e+00  1.493202e+08   \n",
      "min                     NaN                NaN  1.000000e+00 -1.000000e+00   \n",
      "25%                     NaN                NaN  4.000000e+00  9.999072e+08   \n",
      "50%                     NaN                NaN  5.000000e+00  1.128298e+09   \n",
      "75%                     NaN                NaN  5.000000e+00  1.269130e+09   \n",
      "max                     NaN                NaN  5.000000e+00  1.362355e+09   \n",
      "\n",
      "       review/summary                                        review/text  \n",
      "count         2999593                                            2999992  \n",
      "unique        1592314                                            2062648  \n",
      "top        Great Book  digital books are perfect and easy to use! The...  \n",
      "freq             6848                                                322  \n",
      "mean              NaN                                                NaN  \n",
      "std               NaN                                                NaN  \n",
      "min               NaN                                                NaN  \n",
      "25%               NaN                                                NaN  \n",
      "50%               NaN                                                NaN  \n",
      "75%               NaN                                                NaN  \n",
      "max               NaN                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use raw string (r) or forward slashes for Windows paths\n",
    "dir_path = r\"C:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\datasets\\extracted\"\n",
    "\n",
    "\n",
    "# Construct full file path safely\n",
    "file_path_rating = os.path.join(dir_path, 'Books_rating.csv')\n",
    "\n",
    "# Load the data (add error handling)\n",
    "try:\n",
    "    df = pd.read_csv(file_path_rating)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(\"\\nData Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Show first 3 rows\n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path_rating}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becf149",
   "metadata": {},
   "source": [
    "# Preprocess for cadidate_generation stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058736f9",
   "metadata": {},
   "source": [
    "#### 1. Keep Only Required Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ffba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Id</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>1882931173</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>0826414346</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>0826414346</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>0826414346</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>0826414346</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          User_id          Id  review/score\n",
       "0   AVCGYZL8FQQTD  1882931173           4.0\n",
       "1  A30TK6U7DNS82R  0826414346           5.0\n",
       "2  A3UH4UZ4RSVO82  0826414346           5.0\n",
       "3  A2MVUWT453QH61  0826414346           4.0\n",
       "4  A22X4XUPKF66MR  0826414346           4.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['User_id', 'Id', 'review/score']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b74a8",
   "metadata": {},
   "source": [
    "#### Split training/testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc64628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number of unique users: 1008973\n",
      "✅ Train users: 807178, interactions: 2515374\n",
      "✅ Test users: 201795, interactions: 484626\n",
      "208140\n",
      "106678\n",
      "221998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Get unique user IDs\n",
    "unique_users = df['User_id'].unique()\n",
    "\n",
    "# Step 2: Split users into train/test\n",
    "train_users, test_users = train_test_split(\n",
    "    unique_users, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Split the full DataFrame based on user membership\n",
    "train_df = df[df['User_id'].isin(train_users)].reset_index(drop=True)\n",
    "test_df = df[df['User_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "# Print result stats\n",
    "print(f\"✅ Number of unique users: {len(unique_users)}\")\n",
    "print(f\"✅ Train users: {len(train_users)}, interactions: {len(train_df)}\")\n",
    "print(f\"✅ Test users: {len(test_users)}, interactions: {len(test_df)}\")\n",
    "print(len(train_df[\"Id\"].unique()))\n",
    "print(len(test_df[\"Id\"].unique()))\n",
    "print(len(df[\"Id\"].unique()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03abb2",
   "metadata": {},
   "source": [
    "#### Build sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dc0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcef3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSparseMatrix(df):\n",
    "    #  Encode user and item IDs\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    df['user_idx'] = user_encoder.fit_transform(df['User_id'])\n",
    "    df['item_idx'] = item_encoder.fit_transform(df['Id'])\n",
    "\n",
    "    # Build sparse matrix\n",
    "    sparse_matrix = csr_matrix(\n",
    "        (df['review/score'], (df['user_idx'], df['item_idx']))\n",
    "    )\n",
    "\n",
    "    print(f\"Sparse matrix shape: {sparse_matrix.shape}\")\n",
    "    print(f\"Non-zero entries: {sparse_matrix.nnz}\")\n",
    "\n",
    "    return sparse_matrix, user_encoder, item_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7b809d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix shape: (807178, 208140)\n",
      "Non-zero entries: 2001083\n",
      "Sparse matrix and encoders saved.\n",
      "Sparse matrix shape: (201795, 106678)\n",
      "Non-zero entries: 477350\n",
      "Sparse matrix and encoders saved.\n",
      "Sparse matrix shape: (1008973, 221998)\n",
      "Non-zero entries: 2478433\n",
      "Sparse matrix and encoders saved.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "def save_sparse_matrix(sparse_matrix, user_encoder, item_encoder, save_name, output_dir='../datasets/processed'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the matrix and encoders\n",
    "    with open(os.path.join(output_dir, save_name), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'matrix': sparse_matrix,\n",
    "            'user_encoder': user_encoder,\n",
    "            'item_encoder': item_encoder\n",
    "        }, f)\n",
    "\n",
    "    print(\"Sparse matrix and encoders saved.\")\n",
    "\n",
    "sparse_matrix_train, user_encoder_train, item_encoder_train = BuildSparseMatrix(train_df)\n",
    "save_sparse_matrix(sparse_matrix_train, user_encoder_train, item_encoder_train, 'sparse_matrix_train.pkl')\n",
    "\n",
    "sparse_matrix_test, user_encoder_test, item_encoder_test = BuildSparseMatrix(test_df)\n",
    "save_sparse_matrix(sparse_matrix_test, user_encoder_test, item_encoder_test, 'sparse_matrix_test.pkl')\n",
    "\n",
    "sparse_matrix, user_encoder, item_encoder = BuildSparseMatrix(df)\n",
    "save_sparse_matrix(sparse_matrix, user_encoder, item_encoder, 'sparse_matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfcaca",
   "metadata": {},
   "source": [
    "#### Preprocess review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b504fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness',\n",
       "       'review/score', 'review/time', 'review/summary', 'review/text',\n",
       "       'user_idx', 'item_idx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d3d8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          This is only for Julie Strain fans. It's a col...\n",
       "1          I don't care much for Dr. Seuss but after read...\n",
       "2          If people become the books they read and if \"t...\n",
       "3          Theodore Seuss Geisel (1904-1991), aka &quot;D...\n",
       "4          Philip Nel - Dr. Seuss: American IconThis is b...\n",
       "                                 ...                        \n",
       "2999995    This is an extremely difficult book to digest,...\n",
       "2999996    This is pretty interesting. Collingwood seems ...\n",
       "2999997    This is a good book but very esoteric. \"What i...\n",
       "2999998    My daughter, a freshman at Indiana University,...\n",
       "2999999    The guy has a few good ideas but, reader, bewa...\n",
       "Name: review/text, Length: 3000000, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review/text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4f0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 7.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/14.9 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.8/14.9 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.3/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.7/14.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.7/14.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.1/14.9 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/14.9 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.8/2.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.3/6.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.9/6.2 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\n",
      "   ----------------------------------------  0/29 [cymem]\n",
      "   ----------------------------------------  0/29 [cymem]\n",
      "   - --------------------------------------  1/29 [wrapt]\n",
      "   - --------------------------------------  1/29 [wrapt]\n",
      "   - --------------------------------------  1/29 [wrapt]\n",
      "   - --------------------------------------  1/29 [wrapt]\n",
      "   - --------------------------------------  1/29 [wrapt]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   -- -------------------------------------  2/29 [wasabi]\n",
      "   ---- -----------------------------------  3/29 [typing-inspection]\n",
      "   ---- -----------------------------------  3/29 [typing-inspection]\n",
      "   ----- ----------------------------------  4/29 [spacy-loggers]\n",
      "   ----- ----------------------------------  4/29 [spacy-loggers]\n",
      "   ------ ---------------------------------  5/29 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/29 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/29 [spacy-legacy]\n",
      "   -------- -------------------------------  6/29 [shellingham]\n",
      "   -------- -------------------------------  6/29 [shellingham]\n",
      "   -------- -------------------------------  6/29 [shellingham]\n",
      "   -------- -------------------------------  6/29 [shellingham]\n",
      "   --------- ------------------------------  7/29 [pydantic-core]\n",
      "   --------- ------------------------------  7/29 [pydantic-core]\n",
      "   ----------- ----------------------------  8/29 [murmurhash]\n",
      "   ----------- ----------------------------  8/29 [murmurhash]\n",
      "   ----------- ----------------------------  8/29 [murmurhash]\n",
      "   ------------ ---------------------------  9/29 [mdurl]\n",
      "   ------------ ---------------------------  9/29 [mdurl]\n",
      "   ------------ ---------------------------  9/29 [mdurl]\n",
      "   ------------ ---------------------------  9/29 [mdurl]\n",
      "   ------------- -------------------------- 10/29 [marisa-trie]\n",
      "   ------------- -------------------------- 10/29 [marisa-trie]\n",
      "   ------------- -------------------------- 10/29 [marisa-trie]\n",
      "   ------------- -------------------------- 10/29 [marisa-trie]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   --------------- ------------------------ 11/29 [cloudpathlib]\n",
      "   ---------------- ----------------------- 12/29 [click]\n",
      "   ---------------- ----------------------- 12/29 [click]\n",
      "   ---------------- ----------------------- 12/29 [click]\n",
      "   ----------------- ---------------------- 13/29 [catalogue]\n",
      "   ----------------- ---------------------- 13/29 [catalogue]\n",
      "   ------------------- -------------------- 14/29 [blis]\n",
      "   ------------------- -------------------- 14/29 [blis]\n",
      "   -------------------- ------------------- 15/29 [annotated-types]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [srsly]\n",
      "   ----------------------- ---------------- 17/29 [smart-open]\n",
      "   ----------------------- ---------------- 17/29 [smart-open]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [pydantic]\n",
      "   -------------------------- ------------- 19/29 [preshed]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   --------------------------- ------------ 20/29 [markdown-it-py]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ---------------------------- ----------- 21/29 [language-data]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------- -------- 23/29 [langcodes]\n",
      "   ------------------------------- -------- 23/29 [langcodes]\n",
      "   ---------------------------------- ----- 25/29 [typer]\n",
      "   ---------------------------------- ----- 25/29 [typer]\n",
      "   ---------------------------------- ----- 25/29 [typer]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   ---------------------------------------- 29/29 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b6b547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "dir_path = r\"C:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\datasets\\extracted\"\n",
    "# Construct full file path safely\n",
    "file_path_rating = os.path.join(dir_path, 'books_data.csv')\n",
    "\n",
    "# Load the data (add error handling)\n",
    "try:\n",
    "    df = pd.read_csv(file_path_rating)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path_rating}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc81b7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>ratingsCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Julie Strain']</td>\n",
       "      <td>http://books.google.com/books/content?id=DykPA...</td>\n",
       "      <td>http://books.google.nl/books?id=DykPAAAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>http://books.google.nl/books?id=DykPAAAACAAJ&amp;d...</td>\n",
       "      <td>['Comics &amp; Graphic Novels']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>Philip Nel takes a fascinating look into the k...</td>\n",
       "      <td>['Philip Nel']</td>\n",
       "      <td>http://books.google.com/books/content?id=IjvHQ...</td>\n",
       "      <td>http://books.google.nl/books?id=IjvHQsCn_pgC&amp;p...</td>\n",
       "      <td>A&amp;C Black</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>http://books.google.nl/books?id=IjvHQsCn_pgC&amp;d...</td>\n",
       "      <td>['Biography &amp; Autobiography']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>This resource includes twelve principles in un...</td>\n",
       "      <td>['David R. Ray']</td>\n",
       "      <td>http://books.google.com/books/content?id=2tsDA...</td>\n",
       "      <td>http://books.google.nl/books?id=2tsDAAAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>http://books.google.nl/books?id=2tsDAAAACAAJ&amp;d...</td>\n",
       "      <td>['Religion']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0         Its Only Art If Its Well Hung!   \n",
       "1               Dr. Seuss: American Icon   \n",
       "2  Wonderful Worship in Smaller Churches   \n",
       "\n",
       "                                         description           authors  \\\n",
       "0                                                NaN  ['Julie Strain']   \n",
       "1  Philip Nel takes a fascinating look into the k...    ['Philip Nel']   \n",
       "2  This resource includes twelve principles in un...  ['David R. Ray']   \n",
       "\n",
       "                                               image  \\\n",
       "0  http://books.google.com/books/content?id=DykPA...   \n",
       "1  http://books.google.com/books/content?id=IjvHQ...   \n",
       "2  http://books.google.com/books/content?id=2tsDA...   \n",
       "\n",
       "                                         previewLink  publisher publishedDate  \\\n",
       "0  http://books.google.nl/books?id=DykPAAAACAAJ&d...        NaN          1996   \n",
       "1  http://books.google.nl/books?id=IjvHQsCn_pgC&p...  A&C Black    2005-01-01   \n",
       "2  http://books.google.nl/books?id=2tsDAAAACAAJ&d...        NaN          2000   \n",
       "\n",
       "                                            infoLink  \\\n",
       "0  http://books.google.nl/books?id=DykPAAAACAAJ&d...   \n",
       "1  http://books.google.nl/books?id=IjvHQsCn_pgC&d...   \n",
       "2  http://books.google.nl/books?id=2tsDAAAACAAJ&d...   \n",
       "\n",
       "                      categories  ratingsCount  \n",
       "0    ['Comics & Graphic Novels']           NaN  \n",
       "1  ['Biography & Autobiography']           NaN  \n",
       "2                   ['Religion']           NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0f26102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Philip Nel takes a fascinating look into the key aspects of Seuss\\'s career - his poetry, politics, art, marketing, and place in the popular imagination.\" \"Nel argues convincingly that Dr. Seuss is one of the most influential poets in America. His nonsense verse, like that of Lewis Carroll and Edward Lear, has changed language itself, giving us new words like \"nerd.\" And Seuss\\'s famously loopy artistic style - what Nel terms an \"energetic cartoon surrealism\" - has been equally important, inspiring artists like filmmaker Tim Burton and illustrator Lane Smith. --from back cover'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76c1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a49764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\documents\\github\\two-stages-recommendation-system\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.5 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/10.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.6/10.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.9/10.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/14 [urllib3]\n",
      "   ----------------------------------------  0/14 [urllib3]\n",
      "   -- -------------------------------------  1/14 [tqdm]\n",
      "   -- -------------------------------------  1/14 [tqdm]\n",
      "   ----- ----------------------------------  2/14 [safetensors]\n",
      "   -------- -------------------------------  3/14 [regex]\n",
      "   ----------- ----------------------------  4/14 [pyyaml]\n",
      "   -------------- -------------------------  5/14 [idna]\n",
      "   ----------------- ----------------------  6/14 [fsspec]\n",
      "   ----------------- ----------------------  6/14 [fsspec]\n",
      "   ----------------- ----------------------  6/14 [fsspec]\n",
      "   ----------------- ----------------------  6/14 [fsspec]\n",
      "   ----------------- ----------------------  6/14 [fsspec]\n",
      "   -------------------- -------------------  7/14 [filelock]\n",
      "   ---------------------- -----------------  8/14 [charset_normalizer]\n",
      "   ------------------------- --------------  9/14 [certifi]\n",
      "   ---------------------------- ----------- 10/14 [requests]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ------------------------------- -------- 11/14 [huggingface-hub]\n",
      "   ---------------------------------- ----- 12/14 [tokenizers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [transformers]\n",
      "   ---------------------------------------- 14/14 [transformers]\n",
      "\n",
      "Successfully installed certifi-2025.4.26 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 huggingface-hub-0.33.0 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model = \"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd66a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: sadness\n",
      "score: 0.9983229041099548\n"
     ]
    }
   ],
   "source": [
    "prediction = classifier(\"today i'm sad\")\n",
    "for key, value in prediction[0].items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec5844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize emotion classifier\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    truncation=True  # Automatically handles long texts\n",
    ")\n",
    "\n",
    "def detect_emotion(text):\n",
    "    try:\n",
    "        result = emotion_classifier(text[:1000])[0]  # Safely truncate\n",
    "        return result['label']\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Apply to dataframe (first 1000 rows for testing)\n",
    "df['emotion'] = df['review/text'].head(1000).apply(detect_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc01eda6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['review/text', 'emotion'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreview/text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memotion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Two-stages-recommendation-system\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['review/text', 'emotion'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df[['review/text', 'emotion']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f51f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
